{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MATH4280_Code<a id='top'></a>\n",
    "- <a href='#section1'>Ch 1 - Basics</a>\n",
    "- <a href='#section2'>Ch 2 - NumPy</a>\n",
    "- <a href='#section3'>Ch 3 - Matplotlib</a>\n",
    "- <a href='#section4'>Ch 4 - Pandas</a>\n",
    "- <a href='#section5'>Ch 5 - SVD and PCA</a>\n",
    "- <a href='#section6'>Ch 6 - Compressed Sensing and RPCA</a>\n",
    "- <a href='#section7'>Ch 7 - Clustering and LCA</a>\n",
    "- <a href='#section8'>Ch 8 - SVM</a>\n",
    "- <a href='#section9'>Ch 9 - Neural Networks</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ch 1 - Basics<a id='section1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random-number generation\n",
    "import random\n",
    "random.seed(32)\n",
    "random.randrange(1, 7)\n",
    "random.randint(-10000,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f-string\n",
    "print(f'Face{\"Frequency\":>13}')\n",
    "print(f'{1:>4}{frequency1:>13}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing statistics\n",
    "import statistics as stats\n",
    "stats.mean([1, 2, 3])\n",
    "stats.median([1, 2, 3])\n",
    "stats.mode([1, 2, 3])\n",
    "stats.variance([1, 2, 3])\n",
    "stats.stdev([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rounding\n",
    "round(x,4) #round off to 4 decimal places"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#top'>To Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ch 2 - NumPy<a id='section2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arrays\n",
    "arr[:2, 2:4] # extract the first 2 rows and columns\n",
    "arr.mean()\n",
    "arr.max()\n",
    "arr.min()\n",
    "np.nan\n",
    "np.inf\n",
    "\n",
    "arr.reshape(4, 3)\n",
    "arr.flatten() #does not change parent\n",
    "arr.ravel() #change parent also\n",
    "\n",
    "np.random.rand(2,2)\n",
    "\n",
    "#np.where\n",
    "np.where(arr_rand > 5) #locate the positions where the condition is true\n",
    "np.where(doublediff == -2)[0].item(0)\n",
    "\n",
    "#np.unique\n",
    "np.unique([1,1,2,2,3,3]) #return unique elements\n",
    "np.unique(a, axis=0) #return unique rows\n",
    "u, counts = np.unique(arr_rand, return_counts=True) #return the unique items and their counts\n",
    "u, indices = np.unique(a, return_index=True) #return the unique items and their indices\n",
    "\n",
    "u, indices = np.unique(a, return_inverse=True) #return the unique items and the indices of the unique array \n",
    "u[indices] #reconstruct the input array from the unique values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#top'>To Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ch 3 - Matplotlib<a id='section3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plots\n",
    "plt.plot()\n",
    "plt.title('')\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')\n",
    "plt.xlim(0, 6)\n",
    "plt.ylim(0, 6)\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig()\n",
    "\n",
    "#subplots\n",
    "plt.figure(figsize=(10,4), dpi=120)\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,4), sharey=True, dpi=120)\n",
    "plt.subplot(1,2,1)  #(nRows, nColumns, axes number to plot)\n",
    "\n",
    "#text and annotations\n",
    "for i in range(6):\n",
    "    plt.text(frequencies[i], i+1, frequencies[i], ha='left')\n",
    "plt.annotate('Peaks', xy=(90/57.2985, 1.0), xytext=(90/57.2985, 1.5),\n",
    "             bbox=dict(boxstyle='square', fc='green', linewidth=0.1),\n",
    "             arrowprops=dict(facecolor='green', shrink=0.01, width=0.1), \n",
    "             fontsize=12, color='white', horizontalalignment='center')\n",
    "plt.setp(patches[modeg], 'facecolor', 'r')\n",
    "\n",
    "#scatter plots\n",
    "plt.scatter('area', 'poptotal', data=midwest, s='dot_size', c='popdensity', cmap='Reds', edgecolors='black', linewidths=.5)\n",
    "plt.colorbar()\n",
    "plt.scatter(X[:, 0], X[:, 1], c=linear_pred, cmap='winter') #c: labels, cmap: colors for labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#top'>To Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ch 4 - Pandas<a id='section4'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#series and dataframe\n",
    "srs = pd.Series([1,2,3])\n",
    "df = pd.DataFrame(data, columns=list('abcde'))\n",
    "new_df = pd.DataFrame(df, columns=['Sex', 'Under 1']) #create new dataframe by selecting multiple columns\n",
    "\n",
    "df.loc['index name'] #select a column (label based)\n",
    "df.iloc[:3] #select columns (index based)\n",
    "df.columns\n",
    "df.head()\n",
    "df.info()\n",
    "srs3 = srs1.reindex(['China', 'India', 'Malaysia', 'USA', 'Brazil', 'Pakistan', 'England'], fill_value=0) #reindex\n",
    "\n",
    "#data wrangling\n",
    "df2 = df.copy()\n",
    "df.drop('target', axis=1) #drop a column\n",
    "pd.merge(df1, df2, on='subject_id') #merge different dataframes which share a key variable\n",
    "pd.concat([df1, df2]) #combine dataframes across columns or rows\n",
    "pd.DataFrame.join(df1, df2) #join two dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#top'>To Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ch 5 - SVD and PCA<a id='section5'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#singular value decomposition (SVD)\n",
    "import numpy as np\n",
    "U, s, VT = np.linalg.svd(A)\n",
    "\n",
    "#reconstuction\n",
    "Sigma = np.diag(s) #create n x n Sigma matrix\n",
    "Sigma = np.zeros(A.shape)\n",
    "Sigma[:A.shape[1], :A.shape[1]] = np.diag(s) #create m x n Sigma matrix\n",
    "B = U@Sigma@VT\n",
    "\n",
    "#rank-k approximation\n",
    "reconst_matrix = np.dot(U[:,:k], np.dot(np.diag(s[:k]), VT[:k,:]))\n",
    "\n",
    "#compress color images\n",
    "original_shape = image.shape\n",
    "image_reshaped = image.reshape((original_shape[0],original_shape[1]*3))\n",
    "U,s,VT = np.linalg.svd(image,full_matrices=False)\n",
    "image_reconst = np.dot(U[:,:k],np.dot(np.diag(s[:k]),VT[:k,:]))\n",
    "image_reconst = image_reconst.reshape(original_shape)\n",
    "\n",
    "#pseudoinverse\n",
    "from numpy.linalg import pinv\n",
    "B = pinv(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#principal component analysis (PCA)\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3).fit(X) #X is the data matrix\n",
    "Xtransform_data = pca.transform(X) #transform data\n",
    "pca = PCA(n_components=3).fit(df) #df is the dataframe\n",
    "Xtransform_PCA = pca.transform(df) \n",
    "\n",
    "#access values and vectors\n",
    "print(pca.components_)\n",
    "print(pca.explained_variance_)\n",
    "\n",
    "#model training\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "pca = PCA().fit(X_train)\n",
    "\n",
    "#plot the cumulative sum of the explained variance\n",
    "plt.figure(figsize=(18, 7))\n",
    "plt.plot(pca.explained_variance_ratio_.cumsum(), lw=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#top'>To Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ch 6 - Compressed Sensing and RPCA <a id='section6'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as spopt\n",
    "import scipy.fftpack as spfft\n",
    "import scipy.ndimage as spimg\n",
    "import scipy.misc\n",
    "\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install cvxpy\n",
    "import cvxpy as cvx #convex optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reconstruction of signal\n",
    "# sum of two sinusoids\n",
    "n = 5000\n",
    "t = np.linspace(0, 1/8, n)\n",
    "y = np.sin(1394 * np.pi * t) + np.sin(3266 * np.pi * t)\n",
    "yt = spfft.dct(y, norm='ortho')\n",
    "\n",
    "#plot the original signal and its dct\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(t,y)\n",
    "plt.xlim(0,0.12)\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(t,y)\n",
    "plt.xlim(0,0.02)\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('y')\n",
    "\n",
    "k=np.linspace(0, n-1, n)\n",
    "plt.subplot(2,2,3)\n",
    "plt.plot(k,yt)\n",
    "plt.xlim(0,5000)\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('yt')\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.plot(k,yt)\n",
    "plt.xlim(0,500)\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('yt')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# extract small sample of signal\n",
    "m = 500 # 10% sample\n",
    "ri = np.random.choice(n, m, replace=False) # random sample of indices\n",
    "ri.sort() # sorting not strictly necessary, but convenient for plotting\n",
    "t2 = t[ri]\n",
    "y2 = y[ri]\n",
    "\n",
    "#plot the temporal signal\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(t,y)\n",
    "plt.scatter(t2,y2,color='r')\n",
    "plt.xlim(0,0.025)\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(t2,y2,'r')\n",
    "plt.xlim(0,0.025)\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# create idct matrix operator\n",
    "A = spfft.idct(np.identity(n), norm='ortho', axis=0)\n",
    "A = A[ri]\n",
    "\n",
    "# do L1 optimization\n",
    "vx = cvx.Variable(n)\n",
    "objective = cvx.Minimize(cvx.norm(vx, 1))\n",
    "constraints = [A*vx == y2]\n",
    "prob = cvx.Problem(objective, constraints)\n",
    "result = prob.solve(verbose=True, solver=cvx.OSQP)\n",
    "\n",
    "# reconstruct signal\n",
    "x = np.array(vx.value)\n",
    "x = np.squeeze(x)\n",
    "sig = spfft.idct(x, norm='ortho', axis=0)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.subplot(2,2,3)\n",
    "plt.plot(t,y)\n",
    "plt.xlim(0,0.025)\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('y')\n",
    "plt.title('Original')\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.plot(t,sig)\n",
    "plt.xlim(0,0.025)\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('y')\n",
    "plt.title('Reconstructed')\n",
    "\n",
    "k=np.linspace(0, n-1, n)\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(k,yt)\n",
    "plt.xlim(100,500)\n",
    "plt.ylim(-40,40)\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('yt')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(k,vx.value)\n",
    "plt.xlim(100,500)\n",
    "plt.ylim(-40,40)\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('yt')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#robust pca\n",
    "from r_pca import *\n",
    "\n",
    "# generate low rank synthetic data\n",
    "N = 100\n",
    "num_groups = 3\n",
    "num_values_per_group = 40\n",
    "p_missing = 0.2\n",
    "\n",
    "Ds = []\n",
    "for k in range(num_groups):\n",
    "    d = np.ones((N, num_values_per_group)) * (k + 1) * 10\n",
    "    Ds.append(d)\n",
    "D = np.hstack(Ds)\n",
    "\n",
    "# decimate 20% of data \n",
    "n1, n2 = D.shape\n",
    "S = np.random.rand(n1, n2)\n",
    "D[S < 0.2] = 0\n",
    "\n",
    "plt.imshow(D)\n",
    "plt.show()\n",
    "\n",
    "# use R_pca to estimate the degraded data as L + S, where L is low rank, and S is sparse\n",
    "rpca = R_pca(D)\n",
    "L, S = rpca.fit(max_iter=10000, iter_print=100)\n",
    "\n",
    "#plot the original image, L and S\n",
    "plt.figure(figsize=(12,7))\n",
    "plt.subplot(1,4,1)\n",
    "plt.imshow(D)\n",
    "plt.clim(0,30)\n",
    "plt.subplot(1,4,2)\n",
    "plt.imshow(L)\n",
    "plt.clim(0,30)\n",
    "plt.subplot(1,4,3)\n",
    "plt.imshow(S)\n",
    "plt.subplot(1,4,4)\n",
    "plt.imshow(S+L)\n",
    "plt.clim(0,30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#top'>To Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ch 7 - Clustering and LCA<a id='section7'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-means clustering\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(df)\n",
    "label = kmeans.predict(df)\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "#scatter plot\n",
    "colors = list(map(lambda x: colmap[x+1], labels))\n",
    "plt.scatter(df['x'], df['y'], color=colors, alpha=0.5, edgecolor='k')\n",
    "for idx, centroid in enumerate(centroids):\n",
    "    plt.scatter(*centroid, color=colmap[idx+1])\n",
    "plt.xlim(0, 80)\n",
    "plt.ylim(0, 80)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agglomerative clustering\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "cluster = AgglomerativeClustering(n_clusters=5, affinity='euclidean', linkage='ward')\n",
    "#affinity: metric used to compute the linkage, ward: minimize the variance\n",
    "cluster.fit_predict(data)\n",
    "cluster.labels_\n",
    "plt.scatter(data[:,0], data[:,1], c=cluster.labels_, cmap='rainbow')\n",
    "\n",
    "#dendrogram\n",
    "import scipy.cluster.hierarchy as shc\n",
    "dend = shc.dendrogram(shc.linkage(data, method='ward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear discriminant analysis (LCA)\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "X_lda = lda.fit_transform(X, y)\n",
    "lda.explained_variance_ratio_\n",
    "\n",
    "#scatter plot\n",
    "plt.xlabel('LD1')\n",
    "plt.ylabel('LD2')\n",
    "plt.scatter(\n",
    "    X_lda[:,0],\n",
    "    X_lda[:,1],\n",
    "    c=y,\n",
    "    cmap='rainbow',\n",
    "    alpha=0.7,\n",
    "    edgecolors='b'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#top'>To Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ch 8 - SVM<a id='section8'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#support vector machine (SVM)\n",
    "from sklearn.svm import LinearSVC\n",
    "svc = LinearSVC()\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svc.predict(X_test)\n",
    "confusion_matrix(y_test, y_pred)\n",
    "\n",
    "#plot decision boundary and support vectors\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='winter');\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "w = svc.coef_[0]\n",
    "a = -w[0] / w[1]\n",
    "xx = np.linspace(xlim[0], xlim[1])\n",
    "yy = a * xx - svc.intercept_[0] / w[1]\n",
    "plt.plot(xx, yy) #decision boundary\n",
    "yy = a * xx - (svc.intercept_[0] - 1) / w[1]\n",
    "plt.plot(xx, yy, 'k--') #support vector\n",
    "yy = a * xx - (svc.intercept_[0] + 1) / w[1]\n",
    "plt.plot(xx, yy, 'k--') #support vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kernel method for svm\n",
    "from sklearn import svm\n",
    "linear = svm.SVC(kernel='linear', C=1, decision_function_shape='ovo').fit(X_train, y_train)\n",
    "rbf = svm.SVC(kernel='rbf', gamma=1, C=1, decision_function_shape='ovo').fit(X_train, y_train) #radial basis function\n",
    "poly = svm.SVC(kernel='poly', degree=3, C=1, decision_function_shape='ovo').fit(X_train, y_train)\n",
    "sig = svm.SVC(kernel='sigmoid', C=1, decision_function_shape='ovo').fit(X_train, y_train)\n",
    "\n",
    "linear_pred = linear.predict(X_test)\n",
    "poly_pred = poly.predict(X_test)\n",
    "rbf_pred = rbf.predict(X_test)\n",
    "sig_pred = sig.predict(X_test)\n",
    "\n",
    "#accuracy\n",
    "accuracy_lin = linear.score(X_test, y_test)\n",
    "accuracy_poly = poly.score(X_test, y_test)\n",
    "accuracy_rbf = rbf.score(X_test, y_test)\n",
    "accuracy_sig = sig.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#top'>To Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ch 9 - Neural Networks<a id='section9'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode string labels to integer values 0 and 1\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "y = LabelEncoder().fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequential model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "#mlp for classification\n",
    "model = Sequential()\n",
    "model.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n",
    "model.add(Dense(8, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense(3, activation='softmax')) #softmax: outputs add up to 1 (output: probability)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) #loss: crossentropy\n",
    "model.fit(X_train, y_train, epochs=150, batch_size=32, verbose=0) #verbose=0: all outputs can be turned off during training\n",
    "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "yhat = model.predict([row])\n",
    "\n",
    "#mlp for regression\n",
    "model = Sequential()\n",
    "model.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n",
    "model.add(Dense(8, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense(1)) #linear/no activation function for regression problem\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse') #loss: mean-squared error\n",
    "model.fit(X_train, y_train, epochs=150, batch_size=32, verbose=0)\n",
    "error = model.evaluate(X_test, y_test, verbose=0)\n",
    "yhat = model.predict([row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functional model\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# define the layers\n",
    "x_in = Input(shape=(8,))\n",
    "x = Dense(10)(x_in)\n",
    "x_out = Dense(1)(x)\n",
    "# define the model\n",
    "model = Model(inputs=x_in, outputs=x_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convolutional neural network (CNN)\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPool2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), activation='relu', kernel_initializer='he_uniform', input_shape=in_shape))\n",
    "model.add(MaxPool2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dropout(0.5)) #avoid overfitting: remove some of the neurons at each step\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) #optimizer: adam version of sgd\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=128, verbose=0)\n",
    "loss, acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "yhat = model.predict(asarray([image]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recurrent neural network (RNN)\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM #long short-term memory\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', kernel_initializer='he_normal', input_shape=(n_steps,1)))\n",
    "model.add(Dense(50, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense(50, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model summary\n",
    "model.summary()\n",
    "\n",
    "#plot learning curves\n",
    "plt.title('Learning Curves')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='val')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(history.history.keys()) #look at the keys e.g. loss, val_loss\n",
    "\n",
    "#save and load model\n",
    "import sys\n",
    "!{sys.executable} -m pip install h5py\n",
    "model.save('model.h5')\n",
    "model = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#top'>To Top</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
